{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com','www','x200b','\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = pd.read_csv('../../DATA/NEW_ALL_RELIGION_FULL_POST.csv')\n",
    "docs = pd.read_csv('../../DATA/NEW_ALL_VIDEO_GAMES.csv')\n",
    "docs = docs[docs.GAME == 'Smite']\n",
    "docs = docs[docs.RELIGION == 'hinduism']\n",
    "# docs = docs.drop_duplicates()\n",
    "# docs = docs[docs['POST/COMMENT']=='POST']\n",
    "docs['CONTENT'] = docs['CONTENT'].astype(str)\n",
    "docs = docs['CONTENT'].tolist()\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 00:40:31,557 - BERTopic - Transformed documents to Embeddings\n",
      "2023-05-03 00:40:35,461 - BERTopic - Reduced dimensionality\n",
      "2023-05-03 00:40:35,489 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "C_V 0.6697646046699008\n"
     ]
    }
   ],
   "source": [
    "# coherence_scores_u = []\n",
    "# coherence_scores_cv = []\n",
    "\n",
    "\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples=5,\n",
    "                        gen_min_span_tree=True,\n",
    "                        prediction_data=False)\n",
    "\n",
    "# for n_topics in range(3,15):\n",
    "#     print(f\"----Number of Topics: {n_topics}-------\")\n",
    "model = BERTopic(\n",
    "    # umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    # top_n_words=5,\n",
    "    # language='english',\n",
    "    # calculate_probabilities=True,\n",
    "    # verbose=True,\n",
    "    # nr_topics=\"auto\"\n",
    ")\n",
    "topics, probs = model.fit_transform(docs)\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": docs,\n",
    "                        \"ID\": range(len(docs)),\n",
    "                        \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in model.get_topic(topic)] \n",
    "            for topic in range(len(set(topics))-1)]\n",
    "\n",
    "coherence_model_cv = CoherenceModel(topics=topic_words, \n",
    "                                texts=tokens, \n",
    "                                corpus=corpus,\n",
    "                                dictionary=dictionary, \n",
    "                                coherence='c_v')\n",
    "coherence_cv = coherence_model_cv.get_coherence()\n",
    "print('C_V',coherence_cv)\n",
    "\n",
    "# coherence_scores_cv.append(coherence_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                     Name\n",
      "0     -1    274                   -1_would_like_god_also\n",
      "1      0    142              0_hindu_gods_god_hindu gods\n",
      "2      1    114                  1_smite_game_would_gods\n",
      "3      2     68                    2_rez_hi rez_hi_thank\n",
      "4      3     59           3_ganesha_lotus_like_obstacles\n",
      "5      4     56  4_lovecraft_lovecraftian_pantheon_would\n",
      "6      5     54                  5_kali_game_china_model\n",
      "7      6     39               6_damage_ability_god_shiva\n",
      "8      7     39            7_pantheon_pantheons_gods_new\n",
      "9      8     38           8_music_bakasura_know_sanskrit\n"
     ]
    }
   ],
   "source": [
    "print(model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart(top_n_topics=40,n_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
